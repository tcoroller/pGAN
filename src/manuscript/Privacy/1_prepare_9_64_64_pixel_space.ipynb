{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataset size (9, 64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving dataset in reduced size (original synthetic size) for training, validation, test and synthetic\n",
    "The hdf5 version of synthetic with labels is the version of data shared with the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import h5py\n",
    "import pickle\n",
    "import umap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Demo information ============\n",
      "- Working directory: /src\n",
      "- Cuda device: cpu\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "if gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "if os.path.basename(os.getcwd()) != 'BDI-imaging':  # change dir to ROOT\n",
    "    os.chdir(\"../../\")\n",
    "sys.path.insert(0, \"src\")\n",
    "    \n",
    "print('============ Demo information ============')\n",
    "print('- Working directory: /{}'.format(os.getcwd().split('/')[-1]))\n",
    "print('- Cuda device: {}'.format(device))\n",
    "print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batchers for each dataset\n",
    "from manuscript.Train.restricted.train_dataset import F2305Dataset, spineNet_split\n",
    "from manuscript.Train.restricted.test_dataset import A2209Dataset\n",
    "from manuscript.Train.restricted.synthetic_dataset import SynthDataset\n",
    "from manuscript.Train.batchers import F2305Batcher, A2209Batcher, SynthBatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (9, 64, 64)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F2305 training fold\n",
    "Reading images from the training set with the batcher and saving them as a hdf5 file with labels. Splitting F2305 into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using F2305...\n"
     ]
    }
   ],
   "source": [
    "image_list = spineNet_split()['train']\n",
    "print(\"Training using F2305...\")\n",
    "traindata_f = F2305Dataset(shape=image_shape)\n",
    "traindata_f.prepare(label_by=\"reader2\", types=('T1',), subset_fraction=1.0, image_list=image_list)\n",
    "\n",
    "vu_loader_f = DataLoader(F2305Batcher(traindata_f.dataset, traindata_f.scan_path), batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591c31bd023342ea8e89c257c0327b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = np.zeros([len(vu_loader_f), 9, 64, 64])\n",
    "train_region = np.zeros([len(vu_loader_f), 1])\n",
    "for index, sample in tqdm(enumerate(vu_loader_f)):\n",
    "    im = sample['im']\n",
    "    region = sample['region']\n",
    "    train_dataset[index,:,:,:] = im\n",
    "    train_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/train_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=train_dataset)\n",
    "h5f.create_dataset('regions', data=train_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F2305 validation fold\n",
    "Reading images from the training set with the batcher and saving them as a hdf5 file with labels.\n",
    "Selecting validation and test folds from prior work. There is no overlap between patient present in each fold. Validation set has no images from patients in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val using F2305...\n"
     ]
    }
   ],
   "source": [
    "image_list = spineNet_split()['val'] + spineNet_split()['test']\n",
    "print(\"Val using F2305...\")\n",
    "valdata = F2305Dataset(shape=image_shape)\n",
    "valdata.prepare(label_by=\"reader2\", types=('T1',), subset_fraction=1.0, image_list=image_list)\n",
    "\n",
    "vu_loader_val = DataLoader(F2305Batcher(valdata.dataset, valdata.scan_path), batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cc176365c2498fa5aed364d2d34b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_dataset = np.zeros([len(vu_loader_val), 9, 64, 64])\n",
    "val_region = np.zeros([len(vu_loader_val), 1])\n",
    "for index, sample in tqdm(enumerate(vu_loader_val)):\n",
    "    im = sample['im']\n",
    "    region = sample['region']\n",
    "    val_dataset[index,:,:,:] = im\n",
    "    val_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/val_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=val_dataset)\n",
    "h5f.create_dataset('regions', data=val_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full A2209 dataset as test set\n",
    "Reading images from A2209 using the batcher and saving them with labels in a hdf5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using A2209...\n"
     ]
    }
   ],
   "source": [
    "testdata = A2209Dataset(shape=image_shape)\n",
    "print(\"Test using A2209...\")\n",
    "testdata.prepare(label_by=\"berlin_clinical\", types=('T1',), subset_fraction=1.0)\n",
    "\n",
    "vu_loader_test = DataLoader(A2209Batcher(testdata.dataset, testdata.scan_path), batch_size=batch_size, \n",
    "                        shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcde14884a3413a87a553ede8c7bdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = np.zeros([len(vu_loader_test), 9, 64, 64])\n",
    "test_region = np.zeros([len(vu_loader_test), 1])\n",
    "for index, sample in tqdm(enumerate(vu_loader_test)):\n",
    "    im = sample['im']\n",
    "    region = sample['region']\n",
    "    test_dataset[index,:,:,:] = im\n",
    "    test_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/test_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=test_dataset)\n",
    "h5f.create_dataset('regions', data=test_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic dataset\n",
    "Reading images from genrated synthetic dataset using the batcher and saving them with labels in a hdf5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synth dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Synth dataset...\")\n",
    "traindata_synth = SynthDataset()\n",
    "\n",
    "vu_loader_synth = DataLoader(SynthBatcher(traindata_synth.dataset, traindata_synth.scan_path), batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_dataset = np.zeros([len(vu_loader_synth), 9, 64, 64])\n",
    "synth_region = np.zeros([len(vu_loader_synth), 1])\n",
    "for index, sample in tqdm(enumerate(vu_loader_synth)):\n",
    "    im = sample['im']\n",
    "    im = interpolate(im, size=image_shape[1:], mode='bicubic')\n",
    "    region = sample['region']\n",
    "    synth_dataset[index,:,:,:] = im\n",
    "    synth_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/synth_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=synth_dataset)\n",
    "h5f.create_dataset('regions', data=synth_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simulate an attack on the synthetic dataset, for this purpose we create a \"candidate dataset\", this dataset is composed of 334 images from the training set (F2305 used in GAN training), 333 from the validation dataset (F2305 not used in GAN training) and 333 from the testing dataset (A2209 not used in training).\n",
    "The goal is to assess if it is to tell if a candidate sample comes from the training set when compared with the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 334 samples from the training set\n",
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/train_set.h5', 'r')\n",
    "candidate_train = h5f['images'][-334:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 333 samples from the validation set\n",
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/val_set.h5', 'r')\n",
    "candidate_val = h5f['images'][-333:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 333 samples from the test set\n",
    "h5f = h5py.File('./manuscript/Privacy/privacy_saves/test_set.h5', 'r')\n",
    "candidate_test = h5f['images'][-333:]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and save\n",
    "candidate = np.concatenate((candidate_train, candidate_val, candidate_test))\n",
    "np.save('./manuscript/Privacy/privacy_saves/candidate.npy', candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = np.load('./manuscript/Privacy/privacy_saves/candidate.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgan-env",
   "language": "python",
   "name": "pgan-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
