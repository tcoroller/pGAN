{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset 9_112_224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepares dataset with (9, 112, 224)\n",
    "Synthetic dataset was produced at (9, 64, 64). The images are interpolated back to (9, 112, 224).\n",
    "We are partly working and interpolating at (9, 112, 224) because the 2D UMAP projections proved to be more human understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Demo information ============\n",
      "- Working directory: /src\n",
      "- Cuda device: cpu\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# Allocate to GPU if available\n",
    "# Start relative path at the /src/ folder\n",
    "\n",
    "gpu = torch.cuda.is_available()\n",
    "if gpu:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "if os.path.basename(os.getcwd()) != 'BDI-imaging':  # change dir to ROOT\n",
    "    os.chdir(\"../../\")\n",
    "sys.path.insert(0, \"src\")\n",
    "\n",
    "print('============ Demo information ============')\n",
    "print('- Working directory: /{}'.format(os.getcwd().split('/')[-1]))\n",
    "print('- Cuda device: {}'.format(device))\n",
    "print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load batchers for each dataset\n",
    "from manuscript.Train.restricted.train_dataset import F2305Dataset, spineNet_split\n",
    "from manuscript.Train.restricted.test_dataset import A2209Dataset\n",
    "from manuscript.Train.restricted.synthetic_dataset import SynthDataset\n",
    "from manuscript.Train.batchers import F2305Batcher, A2209Batcher, SynthBatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "image_shape = (9, 112, 224)\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using F2305...\n"
     ]
    }
   ],
   "source": [
    "image_list = spineNet_split()['train']\n",
    "print(\"Training using F2305...\")\n",
    "traindata_f = F2305Dataset(shape=image_shape)\n",
    "traindata_f.prepare(label_by=\"reader2\", types=('T1',), subset_fraction=1.0, image_list=image_list)\n",
    "\n",
    "vu_loader_f = DataLoader(F2305Batcher(traindata_f.dataset, traindata_f.scan_path), batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626c507152e548b5ba1bb5214f6fd7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initailize array with images and the one with labels\n",
    "train_dataset = np.zeros([len(vu_loader_f), 9, 112, 224])\n",
    "train_region = np.zeros([len(vu_loader_f), 1])\n",
    "\n",
    "for index, sample in tqdm(enumerate(vu_loader_f)):\n",
    "    im = sample['im']\n",
    "    region = sample['region']\n",
    "    train_dataset[index,:,:,:] = im\n",
    "    train_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training set as a hdf5 file format so it can be sliced without having to load everything in memory\n",
    "h5f = h5py.File('./manuscript/Diversity/diversity_saves/train_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=train_dataset)\n",
    "h5f.create_dataset('regions', data=train_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val using F2305...\n"
     ]
    }
   ],
   "source": [
    "image_list = spineNet_split()['val'] + spineNet_split()['test']\n",
    "print(\"Val using F2305...\")\n",
    "valdata = F2305Dataset(shape=image_shape)\n",
    "valdata.prepare(label_by=\"reader2\", types=('T1',), subset_fraction=1.0, image_list=image_list)\n",
    "\n",
    "vu_loader_val = DataLoader(F2305Batcher(valdata.dataset, valdata.scan_path), batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae7b3781bdf431e801905dbbeb6e2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initailize array with images and the one with labels\n",
    "val_dataset = np.zeros([len(vu_loader_val), 9, 112, 224])\n",
    "val_region = np.zeros([len(vu_loader_val), 1])\n",
    "\n",
    "for index, sample in tqdm(enumerate(vu_loader_val)):\n",
    "    im = sample['im']\n",
    "    region = sample['region']\n",
    "    val_dataset[index,:,:,:] = im\n",
    "    val_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"regions\": shape (2205, 1), type \"<f8\">"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save training set as a hdf5 file format so it can be sliced without having to load everything in memory\n",
    "h5f = h5py.File('./manuscript/Diversity/diversity_saves/val_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=val_dataset)\n",
    "h5f.create_dataset('regions', data=val_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test using A2209...\n"
     ]
    }
   ],
   "source": [
    "testdata = A2209Dataset(shape=image_shape)\n",
    "print(\"Test using A2209...\")\n",
    "testdata.prepare(label_by=\"berlin_clinical\", types=('T1',), subset_fraction=1.0)\n",
    "\n",
    "vu_loader_test = DataLoader(A2209Batcher(testdata.dataset, testdata.scan_path), batch_size=batch_size, \n",
    "                        shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2807792d111b4e8e8c19fb762022d50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initailize array with images and the one with labels\n",
    "test_dataset = np.zeros([len(vu_loader_test), 9, 112, 224])\n",
    "test_region = np.zeros([len(vu_loader_test), 1])\n",
    "\n",
    "for index, sample in tqdm(enumerate(vu_loader_test)):\n",
    "    im = sample['im']\n",
    "    region = sample['region']\n",
    "    test_dataset[index,:,:,:] = im\n",
    "    test_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training set as a hdf5 file format so it can be sliced without having to load everything in memory\n",
    "h5f = h5py.File('./manuscript/Diversity/diversity_saves/test_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=test_dataset)\n",
    "h5f.create_dataset('regions', data=test_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synth set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synth dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Synth dataset...\")\n",
    "traindata_synth = SynthDataset()\n",
    "vu_loader_synth = DataLoader(SynthBatcher(traindata_synth.dataset, traindata_synth.scan_path), batch_size=batch_size,\n",
    "                                    shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdf3f28b7694f48937e33545ebed2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load synthetic dataset\n",
    "synth_dataset = np.zeros([len(vu_loader_synth), 9, 112, 224])\n",
    "synth_region = np.zeros([len(vu_loader_synth), 1])\n",
    "for index, sample in tqdm(enumerate(vu_loader_synth)):\n",
    "    im = sample['im']\n",
    "    im = interpolate(im, size=image_shape[1:], mode='bicubic')\n",
    "    region = sample['region']\n",
    "    synth_dataset[index,:,:,:] = im\n",
    "    synth_region[index,:] = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('./manuscript/Diversity/diversity_saves/synth_set.h5', 'w')\n",
    "h5f.create_dataset('images', data=synth_dataset)\n",
    "h5f.create_dataset('regions', data=synth_region)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgan-env",
   "language": "python",
   "name": "pgan-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
