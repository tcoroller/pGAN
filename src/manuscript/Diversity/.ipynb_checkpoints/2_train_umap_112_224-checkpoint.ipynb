{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train UMAP (9, 112, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for training, loading and projecting data with UMAP into 2D\n",
    "We are partly working and interpolating at (9, 112, 224) because the 2D UMAP projections proved to be more human understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "from glob import glob\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training set. UMAP will only be trained on real samples originating from the training set.\n",
    "h5f = h5py.File('./diversity_saves/train_set.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3000 images\n",
    "real_im = h5f['images'][:3000]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training umap using training data. This step can be skipped and if there is already saved umap object.\n",
    "Running this might lead to different results than the one presented in the article. The training data was shuffled and the UMAP dimensionality reduction is sensitve to changes in input distribution.\n",
    "(Varying the number/sampling of the images used in the training might be enough to get embedding space similar to the one in the article, we were able to reproduce similar projection on many different UMAP training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training UMAP\n",
    "trans = umap.UMAP(n_neighbors=20, random_state=10, min_dist = 0.0).fit(real_im[:1500].reshape([1500, 9*112*224]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = './diversity_saves/umap_1500.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained UMAP object using pickle\n",
    "pickle.dump(trans, open(f_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained UMAP object using pickle\n",
    "f_name = './diversity_saves/umap_3000.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'umap.umap_.UMAP'>\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load((open(f_name, 'rb')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projecting umap on a subset unseen training samples and synthetic samples\n",
    "Projection is done for 1000 real (from training) and 1000 synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading other training samples\n",
    "h5f = h5py.File('./diversity_saves/train_set.h5', 'r')\n",
    "real_im_transform = h5f['images'][3000:4000]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the small samples for reproducability\n",
    "u_real = loaded_model.transform(real_im_transform.reshape([1000, 9*112*224]))\n",
    "np.save('./diversity_saves/umap_real_1k_large.npy', u_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('./diversity_saves/synth_set.h5', 'r')\n",
    "synth_im_transform = h5f['images'][:1000]\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_fake = loaded_model.transform(synth_im_transform.reshape([1000, 9*112*224]))\n",
    "np.save('./diversity_saves/umap_synth_1k_large.npy', u_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Umap full data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projecting umap on the whole training and synthetic dataset\n",
    "Projection is done for 7832 real (from training) and 10000 synthetic images\n",
    "Takes very long to run (around 8h for training + synthetic projection; 20 minutes per split of 1000 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b340005649a24913892380be18fbe49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Projecting training data with UMAP\n",
    "\n",
    "# Splitting data into 1000 samples chunk.\n",
    "# Working with the whole data would make the projection lag out and the progress untractable\n",
    "h5f = h5py.File('./diversity_saves/train_set.h5', 'r')\n",
    "splits_1k = len(h5f['images'])//1000\n",
    "len_train = len(h5f['images'])\n",
    "if len_train%1000 != 0:\n",
    "    splits_1k += 1\n",
    "\n",
    "umap_train_full = np.zeros((len_train, 2))\n",
    "\n",
    "for i in tqdm(range(splits_1k)):\n",
    "    if len_train - i*1000 > 1000:\n",
    "        upper_bound = 1000*(i+1)\n",
    "    else:\n",
    "        upper_bound = len_train\n",
    "    temp_im_train = h5f['images'][1000*i:upper_bound]\n",
    "    temp_im_train = np.array(temp_im_train).reshape([len(temp_im_train), 9*112*224])\n",
    "    \n",
    "    umap_train_full[1000*i:upper_bound,:] = loaded_model.transform(temp_im_train)\n",
    "    \n",
    "np.save('./diversity_saves/umap_train_full.npy', umap_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting synthetic data with UMAP\n",
    "\n",
    "# Splitting data into 1000 samples chunk.\n",
    "# Working with the whole data would make the projection lag out and the progress untractable\n",
    "h5f = h5py.File('./diversity_saves/synth_set.h5', 'r')\n",
    "splits_1k = len(h5f['images'])//1000\n",
    "len_synth = len(h5f['images'])\n",
    "if len_synth%1000 != 0:\n",
    "    splits_1k_ += 1\n",
    "\n",
    "umap_synth_full = np.zeros((len_synth, 2))\n",
    "\n",
    "for i in tqdm(range(splits_1k)):\n",
    "    if len_synth - i*1000 > 1000:\n",
    "        upper_bound = 1000*(i+1)\n",
    "    else:\n",
    "        upper_bound = len_synth\n",
    "    temp_im_synth = h5f['images'][1000*i:upper_bound]\n",
    "    temp_im_synth = np.array(temp_im_synth).reshape([len(temp_im_synth), 9*112*224])\n",
    "\n",
    "    umap_synth_full[1000*i:upper_bound,:] = loaded_model.transform(temp_im_synth)\n",
    "np.save('./diversity_saves/umap_synth_full.npy', umap_synth_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (FDA-env)",
   "language": "python",
   "name": "fda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
